{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# web scraping imports\n",
    "from CNN import *\n",
    "from Reuters import *\n",
    "from SeekingAlpha import *\n",
    "from CNBC import *\n",
    "\n",
    "# install nltk, selenium, genism and bs4\n",
    "# install chromedriver and add to path\n",
    "\n",
    "# ntlk imports\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# prettyprint\n",
    "import pprint\n",
    "\n",
    "# genism imports\n",
    "from gensim import corpora,models\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "stopwords = []  # user defined stopwords\n",
    "\n",
    "def process_article(text):\n",
    "# for single article (text string), tokenize and lemmatize data, return list of word stems\n",
    "    sents = sent_tokenize(text)\n",
    "    sents = '. '.join([s.strip().replace(\"\\n\", \"\") for s in sents])\n",
    "    words = [word for word in sents.lower().split()\n",
    "             if word not in STOPWORDS and word.isalnum() and word not in stopwords]\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    words = [wordnet_lemmatizer.lemmatize(i) for i in words]\n",
    "    p_stemmer = PorterStemmer()\n",
    "    words = [p_stemmer.stem(i) for i in words]\n",
    "    return words\n",
    "\n",
    "\n",
    "def get_corpus(text):\n",
    "# for single article (text string), tokenize data and return corpus (list of lists)\n",
    "    a_list = [process_article(text)]\n",
    "    dictionary = corpora.Dictionary(a_list)\n",
    "    return [dictionary.doc2bow(a) for a in a_list]\n",
    "\n",
    "\n",
    "#https://radimrehurek.com/gensim/models/tfidfmodel.html\n",
    "# models.TfidfModel: normalize=True by default,normalize document vectors to unit euclidean length.You can also inject your own function into normalize.\n",
    "#slope=0.65 by default: Parameter required by pivoted document length normalization which determines the slope to which the old normalization can be tilted. This parameter only works when pivot is defined.\n",
    "def tf_idf(corpus):\n",
    "    average_count=[]\n",
    "    for item in corpus:\n",
    "        if len(item) != 0:\n",
    "            average_count.append(np.mean(item,axis=0)[1])\n",
    "        else:\n",
    "            average_count.append(0)\n",
    "    pivot = np.mean(average_count)    \n",
    "    tfidf = models.TfidfModel(corpus,pivot=pivot)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    return corpus_tfidf\n",
    "\n",
    "\n",
    "def get_lda(news_list, num_topics, num_passes):\n",
    "# for list of articles ([date,text string] list), generate lda model\n",
    "    texts = [process_article(a[1]) for a in news_list]\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(a) for a in texts]\n",
    "    \n",
    "    average_count=[]\n",
    "    for item in corpus:\n",
    "        if len(item) != 0:\n",
    "            average_count.append(np.mean(item,axis=0)[1])\n",
    "        else:\n",
    "            average_count.append(0)\n",
    "    pivot = np.mean(average_count)\n",
    "    \n",
    "    tfidf = models.TfidfModel(corpus,pivot=pivot)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    lda = LdaModel(corpus_tfidf,  # list of lists containing tuples (word index, word freq)\n",
    "                   id2word=dictionary,  # change nums back to words\n",
    "                   num_topics=num_topics,  # need to set num topics\n",
    "                   passes=num_passes)\n",
    "    return lda,corpus_tfidf\n",
    " \n",
    "\n",
    "def print_lda(lda, num_words=8):\n",
    "# prints lda model coefficients, user can specify number of words to include for each topic\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    # create prettyprint obj, 8 words for each topic\n",
    "    pp.pprint(lda.print_topics(num_words=8))\n",
    "\n",
    "\n",
    "def get_topic(article_number,corpus_tfidf):\n",
    "# for a single article (text string), given the order of the article on the loaded news list, returns list of relevant topics ordered by likelihood\n",
    "    from operator import itemgetter\n",
    "    #single_corpus = get_corpus(article) \n",
    "#     lda.get_document_topics(new_a[0],minimum_probability=0.05,per_word_topics=False)\n",
    "    return sorted(lda.get_document_topics(corpus_tfidf[article_number],minimum_probability=0, per_word_topics=False),\n",
    "                  key=itemgetter(1), reverse=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "def get_news(ticker,days):\n",
    "# retreives news from websites, may take a few minutes\n",
    "    #get_cnn(ticker,days)\n",
    "    #get_reuters(ticker,days)\n",
    "    #get_seekingalpha(ticker,days)\n",
    "    get_cnbc(ticker,days)\n",
    "\n",
    "def get_date(filename,current_dir):\n",
    "# for given filename and relative directory, return date (datetime format)\n",
    "    try:\n",
    "        date = datetime.datetime.strptime(filename[:19],'%Y-%m-%d_%H.%M.%S')\n",
    "        return date\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "def get_file(filename,current_dir):\n",
    "# for given filename and relative directory, return file content (text string)\n",
    "    date = get_date(filename,current_dir)\n",
    "    if date:\n",
    "        with open(current_dir+\"/\"+filename,\"r\") as f:\n",
    "            text = f.read()\n",
    "        return text\n",
    "\n",
    "def load_news(ticker,days=3):\n",
    "# for given ticker, loads news from database and returns a list of [date,articles]\n",
    "# default parameter: load articles from recent three days\n",
    "    l = []\n",
    "    for news_source in os.listdir(\"news/\"+ticker):\n",
    "        current_dir = \"news/\"+ticker+\"/\"+news_source\n",
    "        for doc in os.listdir(current_dir):\n",
    "            date = get_date(doc,current_dir) \n",
    "            if date and date>=(datetime.datetime.now()-datetime.timedelta(days)):\n",
    "                l.append([date,get_file(doc,current_dir)])\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict():\n",
    "# load negative and positive dictionaries\n",
    "    import os\n",
    "    def get_dict_words(dict_dir):\n",
    "        with open(dict_dir,'r') as f:\n",
    "            words = []\n",
    "            for line in f:\n",
    "                words.append(line.replace('\\n','').lower())\n",
    "        return words\n",
    "    \n",
    "    neg = get_dict_words(os.getcwd() + '\\\\dictionaries\\\\negative.txt')\n",
    "    pos = get_dict_words(os.getcwd() + '\\\\dictionaries\\\\positive.txt')\n",
    "    return neg,pos\n",
    "\n",
    "def emotion_analysis(text):\n",
    "# input: text (string)\n",
    "# output: negative and positive score\n",
    "    neg,pos = load_dict()\n",
    "    from nltk import word_tokenize\n",
    "    results = dict()\n",
    "    c1 = c2 = 0\n",
    "    for word in word_tokenize(text):\n",
    "        word = word.lower()\n",
    "        if word in neg:\n",
    "            c1 += 1\n",
    "        if word in pos:\n",
    "            c2 += 1\n",
    "    n = len(word_tokenize(text))\n",
    "    results['negative'] = c1/n\n",
    "    results['positive'] = c2/n\n",
    "    return results\n",
    "\n",
    "def emotion_analyzer(text_list):\n",
    "# input: list of texts (list)\n",
    "# output: dataframe containing negative and positive scores of each text\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(columns=['negative','positive'])\n",
    "    count = 1\n",
    "    for text in text_list:\n",
    "        a = emotion_analysis(text[1])\n",
    "        df.loc[count] = [a['negative'],a['positive']]\n",
    "        count += 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lda_noun(nouns_list, num_topics, num_passes):\n",
    "# input: list of nouns (list), number of topics (int), number of passes (int)\n",
    "# output: lda model and corpus_tfidf\n",
    "    texts = [process_article_noun(a) for a in nouns_list]\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(a) for a in texts]\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    lda = LdaModel(corpus_tfidf,  # list of lists containing tuples (word index, word freq)\n",
    "                   id2word=dictionary,  # change nums back to words\n",
    "                   num_topics=num_topics,  # need to set num topics\n",
    "                   passes=num_passes)\n",
    "    return lda,corpus_tfidf\n",
    "\n",
    "def process_article_noun(text):\n",
    "# for single article (text string), tokenize and lemmatize data, return list of word stems\n",
    "# for topic analysis on nouns\n",
    "    sents = sent_tokenize(text)\n",
    "    sents = '. '.join([s.strip().replace(\"\\n\", \"\") for s in sents])\n",
    "    words = [word for word in sents.lower().split()\n",
    "             if word not in STOPWORDS and word.isalnum() and word not in stopwords]\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    words = [wordnet_lemmatizer.lemmatize(i) for i in words]\n",
    "    #p_stemmer = PorterStemmer()\n",
    "    #words = [p_stemmer.stem(i) for i in words]\n",
    "    return words\n",
    "\n",
    "def pick_nouns(doc,stopwords):\n",
    "# input: doc (str) and stopwords (list)\n",
    "# output: list of nouns (list)\n",
    "    import nltk\n",
    "    stopwords = [x.upper() for x in stopwords]\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sens = sent_detector.tokenize(doc)\n",
    "    wpos = []\n",
    "    for i in sens:\n",
    "        word_list = nltk.word_tokenize(i)\n",
    "        wpos.extend(nltk.pos_tag(word_list))\n",
    "    result = []\n",
    "    for i in wpos:\n",
    "        if i[1] == 'NNP':\n",
    "            if i[0].upper() not in stopwords:\n",
    "                result.append(i[0])\n",
    "    return result\n",
    "\n",
    "def noun_lda(doc_list,stopwords,numTopic,numPass):\n",
    "# input: list of news (list), stopwords(case does not matter) (list) , number of topics (int) and number of passes (int)\n",
    "# output: lda model and corpus_tfidf\n",
    "    nouns = []\n",
    "    for i in text:\n",
    "        nouns.append(' '.join(pick_nouns(i,stopwords)))\n",
    "    lda,corpus_tfidf = get_lda_noun(nouns, numTopic, numPass)\n",
    "    return lda,corpus_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_term_matrix(texts_list,num_topics,lda,stopwords,type= 'topic_terms'):\n",
    "# input: list of texts (list), number of topics (int), lda model, list of stopwords (list), type of matrix generated (string)\n",
    "# output: matrix (list)\n",
    "    texts = [process_article_noun(a) for a in texts_list]\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(a) for a in texts]\n",
    "    \n",
    "    nouns = []\n",
    "    for i in texts_list:\n",
    "        nouns.append(' '.join(pick_nouns(i,stopwords)))\n",
    "    texts = [process_article_noun(a) for a in nouns]\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(a) for a in texts]\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    \n",
    "    if type == 'document_topics':\n",
    "        dt = []\n",
    "        for i in range(len(texts_list)):\n",
    "            dt.append(get_topic(i,corpus_tfidf))\n",
    "        return dt\n",
    "    if type == 'topic_terms':\n",
    "        tt = []\n",
    "        for k,v in dictionary.items():\n",
    "            pass\n",
    "        word_map = dictionary.id2token\n",
    "        for i in range(num_topics):\n",
    "            tt.append(list(map(lambda x: (word_map[x[0]],x[1]),lda.get_topic_terms(i))))\n",
    "        return tt\n",
    "    \n",
    "def exp_smooth(alpha,text_list):\n",
    "# input: alpha in the exponential smoothing equation (double), list of texts (list)\n",
    "# output: conslidated sentimental scores (tuple of doubles)\n",
    "    neg = []\n",
    "    pos = []\n",
    "    for d in text:\n",
    "        neg.append(emotion_analysis(d)['negative'])\n",
    "        pos.append(emotion_analysis(d)['positive'])\n",
    "    for i in range(len(text)):\n",
    "        if i == 0:\n",
    "            neg_s = neg[len(pos)-i-1]\n",
    "            pos_s = pos[len(pos)-i-1]\n",
    "        else:\n",
    "            neg_s = alpha*neg[len(pos)-i-1] + (1-alpha)*neg_s\n",
    "            pos_s = alpha*pos[len(pos)-i-1] + (1-alpha)*pos_s\n",
    "    return neg_s,pos_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking for win32 chromedriver:2.46 in cache\n",
      "Driver found in C:\\Users\\Lora\\.wdm\\chromedriver\\2.46\\win32/chromedriver.exe\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/amazons-advertising-sales-growth-slowed-dramatically-in-q1.html 2019-04-25 18:00:58.052172\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/amazons-advertising-sales-growth-slowed-dramatically-in-q1.html 2019-04-25 18:00:58.232776\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/reuters-america-update-1-new-york-investigating-facebook-over-storage-of-unauthorized-email-contacts.html 2019-04-25 18:00:58.643837\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/reuters-america-update-3-canada-watchdog-to-seek-court-order-to-force-facebook-to-follow-privacy-laws.html 2019-04-25 18:00:59.062949\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/new-york-attorney-general-to-probe-facebook.html 2019-04-25 17:00:59.240900\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/reuters-america-brief-canada-says-facebook-must-back-up-commitment-to-protect-canadians-personal-data.html 2019-04-25 17:00:59.546074\n",
      "<class 'str'> https://www.thestreet.com/jim-cramer/investings-not-a-game-cramers-mad-money-recap-april-25-14938280?puc=CNBC&cm_ven=CNBC --\n",
      "<class 'str'> https://www.thestreet.com/jim-cramer/investings-not-a-game-cramers-mad-money-recap-april-25-14938280?puc=CNBC&cm_ven=CNBC --\n",
      "<class 'str'> https://www.thestreet.com/investing/stocks/new-york-attorney-general-to-investigate-facebook-s-14938516?puc=CNBC&cm_ven=CNBC --\n",
      "<class 'str'> https://realmoney.thestreet.com/investing/stocks/i-don-t-like-the-initial-reaction-to-the-amazon-and-intel-reports-14938490?puc=CNBC&cm_ven=CNBC --\n",
      "<class 'str'> https://seekingalpha.com/news/3454539-new-york-ag-probe-facebooks-contact-collection-nyt?source=cnbc --\n",
      "<class 'str'> https://seekingalpha.com/article/4256843-want-generate-alpha-try-vc-strategies?source=cnbc --\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/amazons-advertising-sales-growth-slowed-dramatically-in-q1.html 2019-04-25 18:01:00.278672\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/amazons-advertising-sales-growth-slowed-dramatically-in-q1.html 2019-04-25 18:01:00.437218\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/reuters-america-update-1-new-york-investigating-facebook-over-storage-of-unauthorized-email-contacts.html 2019-04-25 18:01:00.564163\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/reuters-america-update-3-canada-watchdog-to-seek-court-order-to-force-facebook-to-follow-privacy-laws.html 2019-04-25 18:01:00.693865\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/new-york-attorney-general-to-probe-facebook.html 2019-04-25 17:01:00.841013\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/reuters-america-brief-canada-says-facebook-must-back-up-commitment-to-protect-canadians-personal-data.html 2019-04-25 17:01:00.961890\n",
      "<class 'str'> http://video.cnbc.com/gallery/?video=7000076689 --\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/reuters-america-us-stocks-snapshot-sp-500-dips-slightly-as-industrials-weigh.html 2019-04-25 17:01:02.801965\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/reuters-america-us-stocks-sp-500-edges-barely-higher-as-industrials-drag.html 2019-04-25 16:01:03.108382\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/here-are-the-picks-in-the-2019-cnbc-stock-draft.html 2019-04-25 14:01:03.336447\n",
      "<class 'str'> http://video.cnbc.com/gallery/?video=7000076641 --\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/reuters-america-us-stocks-sp-500-flat-as-losses-in-industrials-offset-gains-in-facebook-microsoft.html 2019-04-25 15:01:04.257527\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/stocks-making-the-biggest-moves-midday-3m-ups-microsoft-amex-comcast.html 2019-04-25 13:01:04.409534\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/facebook-is-soaring-on-earnings-three-experts-weigh-in.html 2019-04-25 13:01:04.627272\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/cramer-predicts-facebook-stock-will-soon-get-back-to-all-time-highs.html 2019-04-25 13:01:04.873317\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/reuters-america-update-2-canada-watchdog-to-seek-court-order-to-force-facebook-to-follow-privacy-laws.html 2019-04-25 13:01:05.271685\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/reuters-america-us-stocks-weak-industrial-earnings-drag-wall-street-lower.html 2019-04-25 13:01:05.556814\n",
      "<class 'str'> http://video.cnbc.com/gallery/?video=7000076093 --\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/reuters-america-update-1-canada-watchdogs-investigation-finds-facebook-broke-privacy-laws.html 2019-04-25 13:01:06.510053\n",
      "<class 'str'> http://video.cnbc.com/gallery/?video=7000076628 --\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/reuters-america-irish-regulator-opens-inquiry-into-facebook-over-password-storage.html 2019-04-25 12:01:07.936080\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/stocks-arent-out-of-the-woods-even-though-earnings-recession-isnt-happening.html 2019-04-25 11:01:08.127156\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/earnings-recession-is-not-showing-up-in-the-first-quarter-numbers.html 2019-04-25 11:01:08.563647\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/reuters-america-us-stocks-3m-slump-keeps-wall-st-under-pressure-tech-offers-support.html 2019-04-25 11:01:08.983941\n",
      "<class 'str'> http://video.cnbc.com/gallery/?video=7000076625 --\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/microsoft-hits-1-trillion-market-cap-for-the-first-time-on-earnings.html 2019-04-25 11:01:10.009035\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/reuters-america-us-stocks-snapshot-nasdaq-hits-record-high-at-open-on-strong-tech-earnings-3m-weighs-on-dow.html 2019-04-25 10:01:10.392131\n",
      "<class 'str'> http://video.cnbc.com/gallery/?video=7000076603 --\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/snap-hires-first-cmo-kenny-mitchell.html 2019-04-25 10:01:11.471760\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/reuters-america-us-stocks-nasdaq-eyes-record-high-after-strong-tech-earnings-3m-results-hit-dow.html 2019-04-25 10:01:11.851115\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/wall-street-analysts-loved-facebook-earnings-what-a-start-to-2019.html 2019-04-25 09:01:12.110294\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/stocks-making-the-biggest-moves-premarket-comcast-southwest-ups-facebook-more.html 2019-04-25 09:01:12.587527\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/facebook-q1-2019-user-growth-in-europe-is-bouncing-back-despite-gdpr.html 2019-04-25 09:01:12.951284\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/reuters-america-us-stocks-dow-futures-hit-by-3m-results-upbeat-tech-results-support-nasdaq.html 2019-04-25 08:01:13.286186\n",
      "<class 'str'> http://video.cnbc.com/gallery/?video=7000076600 --\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/reuters-america-global-markets-asian-shares-slip-german-korean-data-hurt-risk-appetite.html 2019-04-25 03:01:14.834284\n",
      "<class 'str'> http://video.cnbc.com/gallery/?video=7000076491 --\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/24/reuters-america-update-2-facebook-beats-profit-estimates-sets-aside-3-bln-for-privacy-penalty.html 2019-04-24 17:43:00\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/24/stocks-making-the-biggest-moves-after-hours-facebook-microsoft-tesla-and-more.html 2019-04-24 17:23:00\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/24/reuters-america-us-stocks-wall-street-edges-lower-energy-stocks-fall.html 2019-04-24 16:36:00\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/24/facebook-estimates-up-to-5-billion-loss-in-ftc-privacy-inquiry.html 2019-04-24 16:11:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/reuters-america-update-1-new-york-investigating-facebook-over-storage-of-unauthorized-email-contacts.html 2019-04-25 18:01:18.571001\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/amazons-advertising-sales-growth-slowed-dramatically-in-q1.html 2019-04-25 18:01:18.709163\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/reuters-america-update-3-canada-watchdog-to-seek-court-order-to-force-facebook-to-follow-privacy-laws.html 2019-04-25 18:01:19.095844\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/new-york-attorney-general-to-probe-facebook.html 2019-04-25 17:01:19.257619\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/reuters-america-brief-canada-says-facebook-must-back-up-commitment-to-protect-canadians-personal-data.html 2019-04-25 17:01:19.582504\n",
      "<class 'str'> https://www.cnbc.com/video/2019/04/25/facebook-bans-personality-quizzes-on-its-platforms.html --\n",
      "<class 'str'> https://www.cnbc.com/video/2019/04/25/2019-stock-draft-round-3-macys-lululemon-and-facebook.html --\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/reuters-america-us-stocks-snapshot-sp-500-dips-slightly-as-industrials-weigh.html 2019-04-25 17:01:21.320894\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/reuters-america-us-stocks-sp-500-edges-barely-higher-as-industrials-drag.html 2019-04-25 16:01:21.655130\n",
      "<class 'datetime.datetime'> https://www.cnbc.com/2019/04/25/here-are-the-picks-in-the-2019-cnbc-stock-draft.html 2019-04-25 14:01:21.832717\n",
      "<class 'str'> https://www.thestreet.com/jim-cramer/investings-not-a-game-cramers-mad-money-recap-april-25-14938280?puc=CNBC&cm_ven=CNBC --\n",
      "<class 'str'> https://www.thestreet.com/jim-cramer/investings-not-a-game-cramers-mad-money-recap-april-25-14938280?puc=CNBC&cm_ven=CNBC --\n",
      "<class 'str'> https://www.thestreet.com/investing/stocks/new-york-attorney-general-to-investigate-facebook-s-14938516?puc=CNBC&cm_ven=CNBC --\n",
      "<class 'str'> https://realmoney.thestreet.com/investing/stocks/i-don-t-like-the-initial-reaction-to-the-amazon-and-intel-reports-14938490?puc=CNBC&cm_ven=CNBC --\n",
      "<class 'str'> https://seekingalpha.com/news/3454539-new-york-ag-probe-facebooks-contact-collection-nyt?source=cnbc --\n",
      "<class 'str'> https://seekingalpha.com/article/4256843-want-generate-alpha-try-vc-strategies?source=cnbc --\n",
      "<class 'str'> https://realmoney.thestreet.com/investing/i-m-maintaining-a-bullish-bias-but-i-ve-increased-my-vigilance-14938141?puc=CNBC&cm_ven=CNBC --\n",
      "<class 'str'> https://realmoney.thestreet.com/investing/stocks/facebook-s-focus-on-portal-device-finds-resistance-amid-privacy-issues-14938071?puc=CNBC&cm_ven=CNBC --\n",
      "<class 'str'> https://www.thestreet.com/investing/earnings/microsoft-market-cap-hits-1-trillion-now-what--14937999?puc=CNBC&cm_ven=CNBC --\n",
      "<class 'str'> https://realmoney.thestreet.com/investing/technology/facebook-s-earnings-were-good-but-a-couple-of-disclosures-weren-t-tech-check-14938060?puc=CNBC&cm_ven=CNBC --\n",
      "<class 'str'> https://www.thestreet.com/investing/facebook-s-earnings-surge-could-be-the-start-of-a-bigger-move-14937877?puc=CNBC&cm_ven=CNBC --\n",
      "<class 'str'> https://www.thestreet.com/jim-cramer/investings-not-a-game-cramers-mad-money-recap-april-25-14938280?puc=CNBC&cm_ven=CNBC --\n",
      "<class 'str'> https://www.thestreet.com/investing/stocks/new-york-attorney-general-to-investigate-facebook-s-14938516?puc=CNBC&cm_ven=CNBC --\n",
      "<class 'str'> https://realmoney.thestreet.com/investing/stocks/i-don-t-like-the-initial-reaction-to-the-amazon-and-intel-reports-14938490?puc=CNBC&cm_ven=CNBC --\n",
      "<class 'str'> https://seekingalpha.com/news/3454539-new-york-ag-probe-facebooks-contact-collection-nyt?source=cnbc --\n",
      "<class 'str'> https://seekingalpha.com/article/4256843-want-generate-alpha-try-vc-strategies?source=cnbc --\n",
      "<class 'str'> https://realmoney.thestreet.com/investing/i-m-maintaining-a-bullish-bias-but-i-ve-increased-my-vigilance-14938141?puc=CNBC&cm_ven=CNBC --\n",
      "<class 'str'> https://realmoney.thestreet.com/investing/stocks/facebook-s-focus-on-portal-device-finds-resistance-amid-privacy-issues-14938071?puc=CNBC&cm_ven=CNBC --\n",
      "<class 'str'> https://www.thestreet.com/investing/earnings/microsoft-market-cap-hits-1-trillion-now-what--14937999?puc=CNBC&cm_ven=CNBC --\n",
      "<class 'str'> https://realmoney.thestreet.com/investing/technology/facebook-s-earnings-were-good-but-a-couple-of-disclosures-weren-t-tech-check-14938060?puc=CNBC&cm_ven=CNBC --\n",
      "<class 'str'> https://www.thestreet.com/investing/facebook-s-earnings-surge-could-be-the-start-of-a-bigger-move-14937877?puc=CNBC&cm_ven=CNBC --\n",
      "<class 'str'> javascript:void(0) --\n"
     ]
    }
   ],
   "source": [
    "get_news(\"fb\",5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   (   0,\n",
      "        '0.006*\"advertis\" + 0.004*\"amazon\" + 0.003*\"revenu\" + 0.003*\"growth\" + '\n",
      "        '0.003*\"ad\" + 0.002*\"access\" + 0.002*\"dramat\" + 0.002*\"expect\"'),\n",
      "    (   1,\n",
      "        '0.004*\"canadian\" + 0.004*\"privaci\" + 0.003*\"therrien\" + '\n",
      "        '0.003*\"nasdaq\" + 0.003*\"inform\" + 0.003*\"index\" + 0.003*\"govern\" + '\n",
      "        '0.003*\"500\"')]\n"
     ]
    }
   ],
   "source": [
    "news = load_news(\"fb\")\n",
    "\n",
    "# print lda model\n",
    "lda,corpus_tfidf = get_lda(news, 2, 10)\n",
    "print_lda(lda)\n",
    "\n",
    "# get topic for single document\n",
    "# a = \"'A version of this article first appeared in the Reliable Sources newsletter. You can sign up for free right here.   This is an incredibly difficult time for Alex Trebek, his family members, and the extended Jeopardy! family that spans the globe. Trebek showed tremendous courage by recording a candid video message to fans about his stage 4 pancreatic cancer diagnosis. He even managed to work in a joke about being under contract for three more years. Trebek was diagnosed earlier this week, and his video was released on Wednesday afternoon.  In a time that is all about what is keeping us apart, we got tough news today about someone who has always brought America together, literally for decades, CNN\\'s Chris Cuomo said Wednesday night. I don\\'t care what your race, color, creed, gender, or bank account level, you\\'ve watched Jeopardy. Since 1984 Alex Trebek has been the smartest guy in our living rooms, teaching us, but more importantly, bringing us together. Trebek\\'s show puts facts first, Cuomo said, and we need him, now mo\"\n",
    "# print(get_topic(a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.047414</td>\n",
       "      <td>0.006466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.018801</td>\n",
       "      <td>0.011751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.010135</td>\n",
       "      <td>0.003378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.027356</td>\n",
       "      <td>0.006079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.020525</td>\n",
       "      <td>0.005701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.020525</td>\n",
       "      <td>0.005701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.009634</td>\n",
       "      <td>0.013487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.005282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.009634</td>\n",
       "      <td>0.013487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.006270</td>\n",
       "      <td>0.022727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.011292</td>\n",
       "      <td>0.007528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.005282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.006270</td>\n",
       "      <td>0.022727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.011292</td>\n",
       "      <td>0.007528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.007130</td>\n",
       "      <td>0.012478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.007130</td>\n",
       "      <td>0.012478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.020794</td>\n",
       "      <td>0.011342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.017081</td>\n",
       "      <td>0.007764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.012876</td>\n",
       "      <td>0.010014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.001721</td>\n",
       "      <td>0.003442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.020794</td>\n",
       "      <td>0.011342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.017081</td>\n",
       "      <td>0.007764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.012876</td>\n",
       "      <td>0.010014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.001721</td>\n",
       "      <td>0.003442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.019912</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.010893</td>\n",
       "      <td>0.017429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.044280</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.024324</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.044280</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.024324</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.031008</td>\n",
       "      <td>0.007752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.044280</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.024324</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.031008</td>\n",
       "      <td>0.007752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.012690</td>\n",
       "      <td>0.007614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.012690</td>\n",
       "      <td>0.007614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.029557</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.044280</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.012690</td>\n",
       "      <td>0.007614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.012690</td>\n",
       "      <td>0.007614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.029557</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.044280</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.029557</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.012690</td>\n",
       "      <td>0.007614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.044280</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.016514</td>\n",
       "      <td>0.001835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.016514</td>\n",
       "      <td>0.001835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.029557</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.016514</td>\n",
       "      <td>0.001835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.016514</td>\n",
       "      <td>0.001835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.029557</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.029557</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.016514</td>\n",
       "      <td>0.001835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    negative  positive\n",
       "1   0.047414  0.006466\n",
       "2   0.018801  0.011751\n",
       "3   0.010135  0.003378\n",
       "4   0.027356  0.006079\n",
       "5   0.020525  0.005701\n",
       "6   0.020525  0.005701\n",
       "7   0.009634  0.013487\n",
       "8   0.014085  0.005282\n",
       "9   0.009634  0.013487\n",
       "10  0.006270  0.022727\n",
       "11  0.011292  0.007528\n",
       "12  0.014085  0.005282\n",
       "13  0.000000  0.007463\n",
       "14  0.006270  0.022727\n",
       "15  0.011292  0.007528\n",
       "16  0.000000  0.018405\n",
       "17  0.000000  0.007463\n",
       "18  0.007130  0.012478\n",
       "19  0.000000  0.018405\n",
       "20  0.007130  0.012478\n",
       "21  0.020794  0.011342\n",
       "22  0.017081  0.007764\n",
       "23  0.012876  0.010014\n",
       "24  0.001721  0.003442\n",
       "25  0.020794  0.011342\n",
       "26  0.017081  0.007764\n",
       "27  0.012876  0.010014\n",
       "28  0.001721  0.003442\n",
       "29  0.019912  0.000000\n",
       "30  0.010893  0.017429\n",
       "..       ...       ...\n",
       "61  0.044280  0.000000\n",
       "62  0.024324  0.000000\n",
       "63  0.000000  0.000000\n",
       "64  0.044280  0.000000\n",
       "65  0.024324  0.000000\n",
       "66  0.000000  0.000000\n",
       "67  0.031008  0.007752\n",
       "68  0.044280  0.000000\n",
       "69  0.024324  0.000000\n",
       "70  0.000000  0.000000\n",
       "71  0.031008  0.007752\n",
       "72  0.012690  0.007614\n",
       "73  0.012690  0.007614\n",
       "74  0.029557  0.000000\n",
       "75  0.044280  0.000000\n",
       "76  0.012690  0.007614\n",
       "77  0.012690  0.007614\n",
       "78  0.029557  0.000000\n",
       "79  0.044280  0.000000\n",
       "80  0.029557  0.000000\n",
       "81  0.012690  0.007614\n",
       "82  0.044280  0.000000\n",
       "83  0.016514  0.001835\n",
       "84  0.016514  0.001835\n",
       "85  0.029557  0.000000\n",
       "86  0.016514  0.001835\n",
       "87  0.016514  0.001835\n",
       "88  0.029557  0.000000\n",
       "89  0.029557  0.000000\n",
       "90  0.016514  0.001835\n",
       "\n",
       "[90 rows x 2 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_analyzer(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news = load_news(\"aapl\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = []\n",
    "# for i in news:\n",
    "#     text.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   (   0,\n",
      "        '0.014*\"qualcomm\" + 0.008*\"netflix\" + 0.008*\"foxconn\" + 0.008*\"huawei\" '\n",
      "        '+ 0.008*\"book\" + 0.008*\"display\" + 0.008*\"japan\" + 0.007*\"roku\"'),\n",
      "    (   1,\n",
      "        '0.012*\"acm\" + 0.009*\"dutch\" + 0.008*\"march\" + 0.008*\"qualcomm\" + '\n",
      "        '0.008*\"huawei\" + 0.007*\"sec\" + 0.007*\"google\" + 0.007*\"app\"'),\n",
      "    (   2,\n",
      "        '0.014*\"qualcomm\" + 0.008*\"oled\" + 0.007*\"eps\" + 0.007*\"qcom\" + '\n",
      "        '0.007*\"nasdaq\" + 0.007*\"chesler\" + 0.006*\"pro\" + 0.006*\"macbook\"'),\n",
      "    (   3,\n",
      "        '0.010*\"intel\" + 0.009*\"brussels\" + 0.009*\"thursday\" + '\n",
      "        '0.008*\"qualcomms\" + 0.008*\"qualcomm\" + 0.007*\"project\" + '\n",
      "        '0.007*\"titan\" + 0.007*\"spotifys\"')]\n"
     ]
    }
   ],
   "source": [
    "# print_lda(noun_lda(text,sw,4,10)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
