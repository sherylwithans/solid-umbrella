{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tang\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# web scraping imports\n",
    "from CNN import *\n",
    "from Reuters import *\n",
    "from SeekingAlpha import *\n",
    "\n",
    "# install nltk, selenium, genism and bs4\n",
    "# install chromedriver and add to path\n",
    "\n",
    "# ntlk imports\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# prettyprint\n",
    "import pprint\n",
    "\n",
    "# genism imports\n",
    "from gensim import corpora,models\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "stopwords = []  # user defined stopwords\n",
    "\n",
    "def process_article(text):\n",
    "# for single article (text string), tokenize and lemmatize data, return list of word stems\n",
    "    sents = sent_tokenize(text)\n",
    "    sents = '. '.join([s.strip().replace(\"\\n\", \"\") for s in sents])\n",
    "    words = [word for word in sents.lower().split()\n",
    "             if word not in STOPWORDS and word.isalnum() and word not in stopwords]\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    words = [wordnet_lemmatizer.lemmatize(i) for i in words]\n",
    "    p_stemmer = PorterStemmer()\n",
    "    words = [p_stemmer.stem(i) for i in words]\n",
    "    return words\n",
    "\n",
    "\n",
    "def get_corpus(text):\n",
    "# for single article (text string), tokenize data and return corpus (list of lists)\n",
    "    a_list = [process_article(text)]\n",
    "    dictionary = corpora.Dictionary(a_list)\n",
    "    return [dictionary.doc2bow(a) for a in a_list]\n",
    "\n",
    "\n",
    "def tf_idf(corpus):\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    return corpus_tfidf\n",
    "\n",
    "\n",
    "def get_lda(news_list, num_topics, num_passes):\n",
    "# for list of articles ([date,text string] list), generate lda model\n",
    "    texts = [process_article(a[1]) for a in news_list]\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(a) for a in texts]\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    lda = LdaModel(corpus_tfidf,  # list of lists containing tuples (word index, word freq)\n",
    "                   id2word=dictionary,  # change nums back to words\n",
    "                   num_topics=num_topics,  # need to set num topics\n",
    "                   passes=num_passes)\n",
    "    return lda,corpus_tfidf\n",
    " \n",
    "\n",
    "def print_lda(lda, num_words=8):\n",
    "# prints lda model coefficients, user can specify number of words to include for each topic\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    # create prettyprint obj, 8 words for each topic\n",
    "    pp.pprint(lda.print_topics(num_words=8))\n",
    "\n",
    "\n",
    "def get_topic(article_number):\n",
    "# for a single article (text string), given the order of the article on the loaded news list, returns list of relevant topics ordered by likelihood\n",
    "    from operator import itemgetter\n",
    "    #single_corpus = get_corpus(article) \n",
    "#     lda.get_document_topics(new_a[0],minimum_probability=0.05,per_word_topics=False)\n",
    "    return sorted(lda.get_document_topics(corpus_tfidf[article_number],minimum_probability=0, per_word_topics=False),\n",
    "                  key=itemgetter(1), reverse=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "def get_news(ticker,days):\n",
    "# retreives news from websites, may take a few minutes\n",
    "    get_cnn(ticker,days)\n",
    "    get_reuters(ticker,days)\n",
    "    get_seekingalpha(ticker,days)\n",
    "\n",
    "def get_date(filename,current_dir):\n",
    "# for given filename and relative directory, return date (datetime format)\n",
    "    try:\n",
    "        date = datetime.datetime.strptime(filename[:19],'%Y-%d-%m_%H.%M.%S')\n",
    "        return date\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "def get_file(filename,current_dir):\n",
    "# for given filename and relative directory, return file content (text string)\n",
    "    date = get_date(filename,current_dir)\n",
    "    if date:\n",
    "        with open(current_dir+\"/\"+filename,\"r\") as f:\n",
    "            text = f.read()\n",
    "        return text\n",
    "\n",
    "def load_news(ticker,days=3):\n",
    "# for given ticker, loads news from database and returns a list of [date,articles]\n",
    "# default parameter: load articles from recent three days\n",
    "    l = []\n",
    "    for news_source in os.listdir(\"news/\"+ticker):\n",
    "        current_dir = \"news/\"+ticker+\"/\"+news_source\n",
    "        for doc in os.listdir(current_dir):\n",
    "            date = get_date(doc,current_dir) \n",
    "            if date and date>=(datetime.datetime.now()-datetime.timedelta(days=days)):\n",
    "                l.append([date,get_file(doc,current_dir)])\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict():\n",
    "    import os\n",
    "    def get_dict_words(dict_dir):\n",
    "        with open(dict_dir,'r') as f:\n",
    "            words = []\n",
    "            for line in f:\n",
    "                words.append(line.replace('\\n','').lower())\n",
    "        return words\n",
    "    \n",
    "    neg = get_dict_words(os.getcwd() + '\\\\dictionaries\\\\negative.txt')\n",
    "    pos = get_dict_words(os.getcwd() + '\\\\dictionaries\\\\positive.txt')\n",
    "    return neg,pos\n",
    "\n",
    "def emotion_analysis(text):\n",
    "    neg,pos = load_dict()\n",
    "    from nltk import word_tokenize\n",
    "    results = dict()\n",
    "    c1 = c2 = 0\n",
    "    for word in word_tokenize(text):\n",
    "        word = word.lower()\n",
    "        if word in neg:\n",
    "            c1 += 1\n",
    "        if word in pos:\n",
    "            c2 += 1\n",
    "    n = len(word_tokenize(text))\n",
    "    results['negative'] = c1/n\n",
    "    results['positive'] = c2/n\n",
    "    return results\n",
    "\n",
    "def emotion_analyzer(text_list):\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(columns=['negative','positive'])\n",
    "    count = 1\n",
    "    for text in text_list:\n",
    "        a = emotion_analysis(text[1])\n",
    "        df.loc[count] = [a['negative'],a['positive']]\n",
    "        count += 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_news(\"fb\",5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   (   0,\n",
      "        '0.003*\"data\" + 0.003*\"commiss\" + 0.003*\"european\" + 0.003*\"follow\" + '\n",
      "        '0.003*\"9\" + 0.003*\"fb\" + 0.003*\"estim\" + 0.003*\"user\"'),\n",
      "    (   1,\n",
      "        '0.003*\"snapchat\" + 0.002*\"c\" + 0.002*\"qualcomm\" + 0.002*\"market\" + '\n",
      "        '0.002*\"snap\" + 0.002*\"platform\" + 0.002*\"chip\" + 0.002*\"center\"')]\n"
     ]
    }
   ],
   "source": [
    "news = load_news(\"fb\")\n",
    "\n",
    "# print lda model\n",
    "lda,corpus_tfidf = get_lda(news, 2, 10)\n",
    "print_lda(lda)\n",
    "\n",
    "# get topic for single document\n",
    "# a = \"'A version of this article first appeared in the Reliable Sources newsletter. You can sign up for free right here.   This is an incredibly difficult time for Alex Trebek, his family members, and the extended Jeopardy! family that spans the globe. Trebek showed tremendous courage by recording a candid video message to fans about his stage 4 pancreatic cancer diagnosis. He even managed to work in a joke about being under contract for three more years. Trebek was diagnosed earlier this week, and his video was released on Wednesday afternoon.  In a time that is all about what is keeping us apart, we got tough news today about someone who has always brought America together, literally for decades, CNN\\'s Chris Cuomo said Wednesday night. I don\\'t care what your race, color, creed, gender, or bank account level, you\\'ve watched Jeopardy. Since 1984 Alex Trebek has been the smartest guy in our living rooms, teaching us, but more importantly, bringing us together. Trebek\\'s show puts facts first, Cuomo said, and we need him, now mo\"\n",
    "# print(get_topic(a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.019097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.008392</td>\n",
       "      <td>0.008392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.009636</td>\n",
       "      <td>0.014989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.008571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   negative  positive\n",
       "1  0.010417  0.019097\n",
       "2  0.008392  0.008392\n",
       "3  0.009636  0.014989\n",
       "4  0.014286  0.008571\n",
       "5  0.025000  0.000000\n",
       "6  0.011905  0.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_analyzer(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
